{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function to load images and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_classes(classes_path):\n",
    "    with open(classes_path, 'r') as file:\n",
    "        classes = file.read().splitlines()\n",
    "    return classes\n",
    "\n",
    "def load_dataset(dataset_path, classes):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for folder_name in ['train', 'test']:  # removed 'validation'\n",
    "        folder_path = os.path.join(dataset_path, folder_name)\n",
    "        annotations_path = os.path.join(folder_path, '_annotations.txt')\n",
    "        with open(annotations_path, 'r') as file:\n",
    "            for line in file.readlines():\n",
    "                parts = line.strip().split()\n",
    "                image_path = os.path.join(folder_path, parts[0])\n",
    "                bounding_boxes = parts[1:]\n",
    "                image = load_img(image_path, target_size=(64, 64))\n",
    "                image = img_to_array(image)\n",
    "                images.append(image)\n",
    "                labels_for_image = []\n",
    "                for bbox in bounding_boxes:\n",
    "                    x_min, y_min, x_max, y_max, class_id = map(int, bbox.split(','))\n",
    "                    class_label = classes[class_id]  # convert class ID to class label\n",
    "                    labels_for_image.append([x_min, y_min, x_max, y_max, class_label])\n",
    "                labels.append(labels_for_image)\n",
    "    images = np.array(images)\n",
    "    return images, labels  # labels is a list of lists of bounding boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function to preprocess images and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(images, labels):\n",
    "    images = images.astype('float32') / 255.0\n",
    "    labels = to_categorical(labels)\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_path = r'C:\\Users\\suraj\\OneDrive\\Desktop\\Projects\\internship_applications_project\\website_snapshot_element_identifier\\Website Screenshots.v1-raw.yolov4pytorch\\train\\_classes.txt'\n",
    "classes = load_classes(classes_path)\n",
    "dataset_path = r'C:\\Users\\suraj\\OneDrive\\Desktop\\Projects\\internship_applications_project\\website_snapshot_element_identifier\\Website Screenshots.v1-raw.yolov4pytorch'\n",
    "images, labels = load_dataset(dataset_path, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "input_shape = X_train.shape[1:]\n",
    "num_classes = len(set(class_label for sublist in y_train for _, _, _, _, class_label in sublist))\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "input_dim = X_train.shape[1] * X_train.shape[2] * X_train.shape[3]  # calculate input_dim\n",
    "\n",
    "# Flatten the images\n",
    "X_train = X_train.reshape(X_train.shape[0], input_dim)\n",
    "X_val = X_val.reshape(X_val.shape[0], input_dim)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_shape=(input_dim,), activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If your y_train and y_val are lists of lists where the last element is the class label, you can convert them to NumPy arrays of class labels \n",
    "import numpy as np\n",
    "\n",
    "y_train = np.array([label[-1] for label in y_train])\n",
    "y_val = np.array([label[-1] for label in y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "y_train_labels = y_train[:, -1]\n",
    "y_val_labels = y_val[:, -1]\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y_train_encoded = encoder.fit_transform(y_train_labels)\n",
    "y_val_encoded = encoder.transform(y_val_labels)\n",
    "\n",
    "y_train = to_categorical(y_train_encoded, num_classes=8)\n",
    "y_val = to_categorical(y_val_encoded, num_classes=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "49/49 [==============================] - 1s 9ms/step - loss: 442.3643 - accuracy: 0.1574 - val_loss: 2.0694 - val_accuracy: 0.2021\n",
      "Epoch 2/10\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 2.0611 - accuracy: 0.1788 - val_loss: 2.0510 - val_accuracy: 0.2021\n",
      "Epoch 3/10\n",
      "49/49 [==============================] - 0s 7ms/step - loss: 2.0421 - accuracy: 0.2513 - val_loss: 2.0321 - val_accuracy: 0.2824\n",
      "Epoch 4/10\n",
      "49/49 [==============================] - 0s 7ms/step - loss: 2.0232 - accuracy: 0.3089 - val_loss: 2.0135 - val_accuracy: 0.2824\n",
      "Epoch 5/10\n",
      "49/49 [==============================] - 0s 7ms/step - loss: 2.0043 - accuracy: 0.3089 - val_loss: 1.9949 - val_accuracy: 0.2824\n",
      "Epoch 6/10\n",
      "49/49 [==============================] - 0s 8ms/step - loss: 1.9862 - accuracy: 0.3089 - val_loss: 1.9778 - val_accuracy: 0.2824\n",
      "Epoch 7/10\n",
      "49/49 [==============================] - 0s 7ms/step - loss: 1.9691 - accuracy: 0.3089 - val_loss: 1.9611 - val_accuracy: 0.2824\n",
      "Epoch 8/10\n",
      "49/49 [==============================] - 0s 7ms/step - loss: 1.9527 - accuracy: 0.3089 - val_loss: 1.9453 - val_accuracy: 0.2824\n",
      "Epoch 9/10\n",
      "49/49 [==============================] - 0s 7ms/step - loss: 1.9371 - accuracy: 0.3089 - val_loss: 1.9306 - val_accuracy: 0.2824\n",
      "Epoch 10/10\n",
      "49/49 [==============================] - 0s 7ms/step - loss: 1.9224 - accuracy: 0.3089 - val_loss: 1.9166 - val_accuracy: 0.2824\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x196aca1f990>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 1ms/step - loss: 1.9166 - accuracy: 0.2824\n",
      "Loss: 1.9165962934494019\n",
      "Accuracy: 0.28238341212272644\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_val, y_val)\n",
    "print('Loss:', loss)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 14ms/step\n",
      "Predicted class: [0]\n",
      "[0.17586988 0.0982605  0.1093415  0.1028178  0.1540752  0.08210292\n",
      " 0.11669035 0.16084185]\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Load the image\n",
    "image = Image.open(r'C:\\Users\\suraj\\OneDrive\\Desktop\\Projects\\internship_applications_project\\website_snapshot_element_identifier\\Website Screenshots.v1-raw.yolov4pytorch\\test\\addons_mozilla_org_png.rf.8pWFwr9ZblJ92BQghXBW.jpg')\n",
    "\n",
    "\n",
    "# Resize the image to match the input shape of your model\n",
    "# Your model seems to have been trained on images of size 64x64\n",
    "image = image.resize((64, 64))\n",
    "\n",
    "# Convert the image to a numpy array and normalize the pixel values (if you did so during training)\n",
    "image_array = np.array(image) / 255.0\n",
    "\n",
    "# Add an extra dimension to match the model's input shape (model expects a batch)\n",
    "image_array = np.expand_dims(image_array, axis=0)\n",
    "\n",
    "# Flatten the image\n",
    "image_array = image_array.reshape((1, -1))\n",
    "\n",
    "# Use the model to predict the class of the image\n",
    "predictions = model.predict(image_array)\n",
    "\n",
    "# The predictions are probabilities for each class, so we'll take the class with the highest probability\n",
    "predicted_class = np.argmax(predictions, axis=-1)\n",
    "\n",
    "print(f'Predicted class: {predicted_class}')\n",
    "print(predictions[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
