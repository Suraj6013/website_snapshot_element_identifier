{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torch \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_annotation_file(annotation_path):\n",
    "    annotations = []\n",
    "    with open(annotation_path, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 6:  # Assuming format: image_path x_min,y_min,x_max,y_max,class_id\n",
    "                image_path = parts[0]\n",
    "                bbox_class_info = parts[1].split(',')\n",
    "                bbox_info = [int(coord) for coord in bbox_class_info[:4]]\n",
    "                class_id = int(bbox_class_info[-1])\n",
    "\n",
    "                annotations.append({\n",
    "                    'image_path': image_path,\n",
    "                    'bbox': tuple(bbox_info),\n",
    "                    'class_id': class_id\n",
    "                })\n",
    "    return annotations\n",
    "\n",
    "# Example usage:\n",
    "train_annotations = read_annotation_file(r'C:\\Users\\suraj\\OneDrive\\Desktop\\Projects\\internship_applications_project\\website_snapshot_element_identifier\\Website Screenshots.v1-raw.yolov4pytorch\\test\\_annotations.txt')\n",
    "test_annotations = read_annotation_file(r'C:\\Users\\suraj\\OneDrive\\Desktop\\Projects\\internship_applications_project\\website_snapshot_element_identifier\\Website Screenshots.v1-raw.yolov4pytorch\\train\\_annotations.txt')\n",
    "valid_annotations = read_annotation_file(r'C:\\Users\\suraj\\OneDrive\\Desktop\\Projects\\internship_applications_project\\website_snapshot_element_identifier\\Website Screenshots.v1-raw.yolov4pytorch\\valid\\_annotations.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess data\n",
    "def preprocess_data(image_path, annotation):\n",
    "    try:\n",
    "        print(f\"Processing image: {image_path}\")\n",
    "\n",
    "        # Load the image in color (3 channels)\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "\n",
    "        if image is None:\n",
    "            raise ValueError(f\"Error loading image: {image_path}\")\n",
    "\n",
    "        # Check the number of channels\n",
    "        num_channels = image.shape[-1]\n",
    "\n",
    "        if num_channels == 1:\n",
    "            # Convert grayscale image to RGB format\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "        elif num_channels != 3:\n",
    "            raise ValueError(f\"Error: Image has {num_channels} channels. Expected 3 channels for RGB.\")\n",
    "\n",
    "        # Normalize pixel values to the range [0, 1]\n",
    "        image = image.astype(np.float32) / 255.0\n",
    "\n",
    "        # Extract bounding box coordinates from the annotation\n",
    "        x_min, y_min, x_max, y_max = annotation['bbox']\n",
    "\n",
    "        # Convert bounding box coordinates to the format [x_min, y_min, x_max, y_max]\n",
    "        normalized_bbox = [x_min, y_min, x_max, y_max]\n",
    "\n",
    "        # Add other preprocessing steps if needed\n",
    "\n",
    "        return image, normalized_bbox, annotation['class_id']\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image: {image_path}\")\n",
    "        print(e)\n",
    "        return None, None, None  # Return placeholders if an error occurs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA SPLITTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 0\n",
      "Number of validation samples: 0\n",
      "Number of test samples: 1\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "\n",
    "# Assuming you have a list of annotations where each annotation contains the image path, bbox, and class_id\n",
    "annotations = [...]  # Replace with your actual list of annotations\n",
    "\n",
    "# Shuffle the dataset\n",
    "shuffle(annotations)\n",
    "\n",
    "# Define the proportions for training, validation, and test sets\n",
    "train_size = 0.7  # 70% for training\n",
    "val_size = 0.15   # 15% for validation\n",
    "test_size = 0.15  # 15% for testing\n",
    "\n",
    "# Calculate the split indices\n",
    "num_samples = len(annotations)\n",
    "train_split = int(train_size * num_samples)\n",
    "val_split = train_split + int(val_size * num_samples)\n",
    "\n",
    "# Split the dataset\n",
    "train_annotations = annotations[:train_split]\n",
    "val_annotations = annotations[train_split:val_split]\n",
    "test_annotations = annotations[val_split:]\n",
    "\n",
    "# Print the sizes of each set\n",
    "print(f\"Number of training samples: {len(train_annotations)}\")\n",
    "print(f\"Number of validation samples: {len(val_annotations)}\")\n",
    "print(f\"Number of test samples: {len(test_annotations)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL ARCHITECTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 766, 1022, 32)     896       \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 383, 511, 32)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 381, 509, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 190, 254, 64)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 3088640)           0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               395346048 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 8)                 1032      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 395366472 (1.47 GB)\n",
      "Trainable params: 395366472 (1.47 GB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# Define the input shape based on your image dimensions\n",
    "input_shape = (768, 1024, 3)  # Replace with your actual image dimensions\n",
    "\n",
    "# Create a sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Convolutional layers\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flatten layer to transition from convolutional to dense layers\n",
    "model.add(Flatten())\n",
    "\n",
    "# Dense layers\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))  # Dropout for regularization\n",
    "model.add(Dense(8, activation='softmax'))  # Adjust num_classes based on your task\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print a summary of the model architecture\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\suraj\\\\OneDrive\\\\Desktop\\\\banao_ai\\\\Website Screenshots.v1-raw.yolov4pytorch\\\\valid'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m labels \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Iterate through your dataset directory\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;66;03m# Assuming image filenames contain the corresponding label (e.g., 'label_image.jpg')\u001b[39;00m\n\u001b[0;32m     20\u001b[0m         label \u001b[38;5;241m=\u001b[39m filename\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Modify this based on your naming convention\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\suraj\\\\OneDrive\\\\Desktop\\\\banao_ai\\\\Website Screenshots.v1-raw.yolov4pytorch\\\\valid'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "# Define the path to your dataset\n",
    "dataset_path = r'C:\\Users\\suraj\\OneDrive\\Desktop\\Projects\\internship_applications_project\\website_snapshot_element_identifier\\Website Screenshots.v1-raw.yolov4pytorch'\n",
    "\n",
    "# Placeholder lists for images and labels\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "# Iterate through your dataset directory\n",
    "for filename in os.listdir(dataset_path):\n",
    "    if filename.endswith('.jpg'):\n",
    "        # Assuming image filenames contain the corresponding label (e.g., 'label_image.jpg')\n",
    "        label = filename.split('_')[0]  # Modify this based on your naming convention\n",
    "        image_path = os.path.join(dataset_path, filename)\n",
    "        \n",
    "        # Load the image using OpenCV\n",
    "        image = cv2.imread(image_path)\n",
    "        \n",
    "        # Append the image and label to the lists\n",
    "        images.append(image)\n",
    "        labels.append(label)\n",
    "\n",
    "# Verify the loaded data\n",
    "assert len(images) == len(labels), \"Mismatched number of images and labels\"\n",
    "# Assuming you have lists of images and corresponding labels\n",
    "# Replace [...] with your actual dataset loading code\n",
    "\n",
    "# Verify the loaded data\n",
    "assert len(images) == len(labels), \"Mismatched number of images and labels\"\n",
    "\n",
    "# Print the labels before conversion\n",
    "print(\"Labels before conversion:\", labels)\n",
    "\n",
    "# Convert labels to integers if they are not already\n",
    "try:\n",
    "    labels = [int(label.split('px')[0]) for label in labels]\n",
    "except ValueError:\n",
    "    print(\"Error: Labels contain non-numeric characters or are not convertible to integers.\")\n",
    "\n",
    "# Convert labels to integers if they are numeric, otherwise assign a unique integer to non-numeric labels\n",
    "label_dict = {}\n",
    "numeric_labels = []\n",
    "for label in labels:\n",
    "    try:\n",
    "        numeric_label = int(label)\n",
    "        numeric_labels.append(numeric_label)\n",
    "    except ValueError:\n",
    "        if label not in label_dict:\n",
    "            label_dict[label] = len(label_dict)\n",
    "        numeric_labels.append(label_dict[label])\n",
    "\n",
    "# Print the labels after conversion\n",
    "print(\"Labels after conversion:\", numeric_labels)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "num_classes = len(set(numeric_labels))\n",
    "labels_one_hot = to_categorical(numeric_labels, num_classes=num_classes)\n",
    "\n",
    "\n",
    "\n",
    "# # Print the labels after conversion\n",
    "# print(\"Labels after conversion:\", labels)\n",
    "\n",
    "# # Convert labels to one-hot encoding\n",
    "# num_classes = len(set(labels))\n",
    "# labels_one_hot = to_categorical(labels, num_classes=num_classes)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(images, labels_one_hot, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the sizes of each set\n",
    "print(f\"Number of training samples: {len(X_train)}\")\n",
    "print(f\"Number of validation samples: {len(X_val)}\")\n",
    "print(f\"Number of test samples: {len(y_val)}\")\n",
    "print(finally)\n",
    "\n",
    "\n",
    "# Continue with the rest of your code...\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 766, 1022, 32)     896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 383, 511, 32)      0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 381, 509, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 190, 254, 64)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 3088640)           0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               395346048 \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8)                 1032      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 395366472 (1.47 GB)\n",
      "Trainable params: 395366472 (1.47 GB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Evaluation:\n",
    "\n",
    "Evaluate your trained model on the validation set to assess its generalization performance. Use metrics relevant to your task, such as accuracy, precision, recall, or others.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 33s 3s/step\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "# Assuming X_val is your validation data\n",
    "y_pred = model.predict(np.array(X_val))\n",
    "\n",
    "# Assuming y_val is one-hot encoded, convert it back to labels if needed\n",
    "y_true_labels = np.argmax(y_val, axis=1)\n",
    "\n",
    "# Compute metrics\n",
    "accuracy = accuracy_score(y_true_labels, y_true_labels)\n",
    "precision = precision_score(y_true_labels, y_true_labels, average='weighted')\n",
    "recall = recall_score(y_true_labels, y_true_labels, average='weighted')\n",
    "f1 = f1_score(y_true_labels, y_true_labels, average='weighted')\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Set Evaluation:\n",
    "\n",
    "Once satisfied with the model's performance on the validation set, evaluate it on the test set to obtain a final assessment of its generalization capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load your saved model\n",
    "your_model = load_model(r'C:\\Users\\suraj\\OneDrive\\Desktop\\banao_ai\\your_model.h5')\n",
    "\n",
    "# Enable eager execution\n",
    "your_model.compile(run_eagerly=True)\n",
    "\n",
    "class_names = ['button', 'field', 'heading', 'iframe', 'image', 'label', 'link', 'text']\n",
    "\n",
    "# Sample annotation file format: Assuming CSV format with columns: image_path, xmin, ymin, xmax, ymax, class_label\n",
    "annotation_file = r'C:\\Users\\suraj\\OneDrive\\Desktop\\banao_ai\\Website Screenshots.v1-raw.yolov4pytorch\\test\\_annotations.txt'\n",
    "\n",
    "# Read the annotation file\n",
    "annotations = []\n",
    "with open(annotation_file, 'r') as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split(',')\n",
    "        annotations.append({\n",
    "            'image_path': parts[0],\n",
    "            'xmin': int(parts[1]),\n",
    "            'ymin': int(parts[2]),\n",
    "            'xmax': int(parts[3]),\n",
    "            'ymax': int(parts[4].replace(\" \", \"\")),  # Remove spaces and convert to int\n",
    "            'class_label': int(parts[5])  # Assuming class labels are integers\n",
    "        })\n",
    "\n",
    "# Load your test data (test_image_paths)\n",
    "test_image_paths = [annotation['image_path'] for annotation in annotations]\n",
    "\n",
    "for image_path, annotation in zip(test_image_paths, annotations):\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is not None:\n",
    "        img = img / 255.0\n",
    "        img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "        predictions = your_model.predict(img)\n",
    "\n",
    "        print(f\"Image: {image_path}\")\n",
    "        print(\"Annotations:\", annotation)\n",
    "        print(\"Predictions shape:\", predictions.shape)\n",
    "\n",
    "        if len(predictions) > 0:\n",
    "            print(\"First prediction:\", predictions[0])\n",
    "            confidence_threshold = 0.5\n",
    "            filtered_predictions = [pred for pred in predictions[0] if pred[4] >= confidence_threshold]\n",
    "            print(\"Filtered Predictions shape:\", np.array(filtered_predictions).shape)\n",
    "\n",
    "        # Set a confidence threshold\n",
    "        confidence_threshold = 0.5\n",
    "\n",
    "        # Draw bounding boxes and labels on the image\n",
    "        for detection in predictions[0]:\n",
    "            confidence = detection[0]  # Assuming confidence is in the first position in the array\n",
    "            if confidence > confidence_threshold:\n",
    "                class_index = np.argmax(detection[1:])  # Assuming the class prediction is the highest probability\n",
    "                class_name = class_names[class_index]\n",
    "\n",
    "                # Get the bounding box coordinates from the annotation\n",
    "                xmin, ymin, xmax, ymax = annotation['xmin'], annotation['ymin'], annotation['xmax'], annotation['ymax']\n",
    "\n",
    "                # Draw bounding box\n",
    "                cv2.rectangle(img, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n",
    "\n",
    "                # Draw label\n",
    "                label = f'{class_name}: {confidence:.2f}'\n",
    "                cv2.putText(img, label, (xmin, ymin - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "        # Display the image\n",
    "        plt.imshow(img[0])  # Remove batch dimension before displaying\n",
    "        plt.axis('off')\n",
    "        plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
