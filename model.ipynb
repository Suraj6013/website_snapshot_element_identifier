{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torch \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\suraj\\\\OneDrive\\\\Desktop\\\\banao_ai\\\\Website Screenshots.v1-raw.yolov4pytorch\\\\train\\\\_annotations.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m annotations\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m train_annotations \u001b[38;5;241m=\u001b[39m \u001b[43mread_annotation_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43msuraj\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mOneDrive\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mDesktop\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mbanao_ai\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mWebsite Screenshots.v1-raw.yolov4pytorch\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m_annotations.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m test_annotations \u001b[38;5;241m=\u001b[39m read_annotation_file(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124msuraj\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mOneDrive\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mbanao_ai\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mWebsite Screenshots.v1-raw.yolov4pytorch\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m_annotations.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     22\u001b[0m valid_annotations \u001b[38;5;241m=\u001b[39m read_annotation_file(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124msuraj\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mOneDrive\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mbanao_ai\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mWebsite Screenshots.v1-raw.yolov4pytorch\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m_annotations.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m, in \u001b[0;36mread_annotation_file\u001b[1;34m(annotation_path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_annotation_file\u001b[39m(annotation_path):\n\u001b[0;32m      2\u001b[0m     annotations \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mannotation_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m file:\n\u001b[0;32m      5\u001b[0m             parts \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    284\u001b[0m     )\n\u001b[1;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\suraj\\\\OneDrive\\\\Desktop\\\\banao_ai\\\\Website Screenshots.v1-raw.yolov4pytorch\\\\train\\\\_annotations.txt'"
     ]
    }
   ],
   "source": [
    "def read_annotation_file(annotation_path):\n",
    "    annotations = []\n",
    "    with open(annotation_path, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 6:  # Assuming format: image_path x_min,y_min,x_max,y_max,class_id\n",
    "                image_path = parts[0]\n",
    "                bbox_class_info = parts[1].split(',')\n",
    "                bbox_info = [int(coord) for coord in bbox_class_info[:4]]\n",
    "                class_id = int(bbox_class_info[-1])\n",
    "\n",
    "                annotations.append({\n",
    "                    'image_path': image_path,\n",
    "                    'bbox': tuple(bbox_info),\n",
    "                    'class_id': class_id\n",
    "                })\n",
    "    return annotations\n",
    "\n",
    "# Example usage:\n",
    "train_annotations = read_annotation_file(r'C:\\Users\\suraj\\OneDrive\\Desktop\\banao_ai\\Website Screenshots.v1-raw.yolov4pytorch\\train\\_annotations.txt')\n",
    "test_annotations = read_annotation_file(r'C:\\Users\\suraj\\OneDrive\\Desktop\\banao_ai\\Website Screenshots.v1-raw.yolov4pytorch\\test\\_annotations.txt')\n",
    "valid_annotations = read_annotation_file(r'C:\\Users\\suraj\\OneDrive\\Desktop\\banao_ai\\Website Screenshots.v1-raw.yolov4pytorch\\valid\\_annotations.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess data\n",
    "def preprocess_data(image_path, annotation):\n",
    "    try:\n",
    "        print(f\"Processing image: {image_path}\")\n",
    "\n",
    "        # Load the image in color (3 channels)\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "\n",
    "        if image is None:\n",
    "            raise ValueError(f\"Error loading image: {image_path}\")\n",
    "\n",
    "        # Check the number of channels\n",
    "        num_channels = image.shape[-1]\n",
    "\n",
    "        if num_channels == 1:\n",
    "            # Convert grayscale image to RGB format\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "        elif num_channels != 3:\n",
    "            raise ValueError(f\"Error: Image has {num_channels} channels. Expected 3 channels for RGB.\")\n",
    "\n",
    "        # Normalize pixel values to the range [0, 1]\n",
    "        image = image.astype(np.float32) / 255.0\n",
    "\n",
    "        # Extract bounding box coordinates from the annotation\n",
    "        x_min, y_min, x_max, y_max = annotation['bbox']\n",
    "\n",
    "        # Convert bounding box coordinates to the format [x_min, y_min, x_max, y_max]\n",
    "        normalized_bbox = [x_min, y_min, x_max, y_max]\n",
    "\n",
    "        # Add other preprocessing steps if needed\n",
    "\n",
    "        return image, normalized_bbox, annotation['class_id']\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image: {image_path}\")\n",
    "        print(e)\n",
    "        return None, None, None  # Return placeholders if an error occurs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA SPLITTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 0\n",
      "Number of validation samples: 0\n",
      "Number of test samples: 1\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "\n",
    "# Assuming you have a list of annotations where each annotation contains the image path, bbox, and class_id\n",
    "annotations = [...]  # Replace with your actual list of annotations\n",
    "\n",
    "# Shuffle the dataset\n",
    "shuffle(annotations)\n",
    "\n",
    "# Define the proportions for training, validation, and test sets\n",
    "train_size = 0.7  # 70% for training\n",
    "val_size = 0.15   # 15% for validation\n",
    "test_size = 0.15  # 15% for testing\n",
    "\n",
    "# Calculate the split indices\n",
    "num_samples = len(annotations)\n",
    "train_split = int(train_size * num_samples)\n",
    "val_split = train_split + int(val_size * num_samples)\n",
    "\n",
    "# Split the dataset\n",
    "train_annotations = annotations[:train_split]\n",
    "val_annotations = annotations[train_split:val_split]\n",
    "test_annotations = annotations[val_split:]\n",
    "\n",
    "# Print the sizes of each set\n",
    "print(f\"Number of training samples: {len(train_annotations)}\")\n",
    "print(f\"Number of validation samples: {len(val_annotations)}\")\n",
    "print(f\"Number of test samples: {len(test_annotations)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL ARCHITECTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 766, 1022, 32)     896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 383, 511, 32)      0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 381, 509, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 190, 254, 64)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 3088640)           0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               395346048 \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8)                 1032      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 395366472 (1.47 GB)\n",
      "Trainable params: 395366472 (1.47 GB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# Define the input shape based on your image dimensions\n",
    "input_shape = (768, 1024, 3)  # Replace with your actual image dimensions\n",
    "\n",
    "# Create a sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Convolutional layers\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flatten layer to transition from convolutional to dense layers\n",
    "model.add(Flatten())\n",
    "\n",
    "# Dense layers\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))  # Dropout for regularization\n",
    "model.add(Dense(8, activation='softmax'))  # Adjust num_classes based on your task\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print a summary of the model architecture\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels before conversion: ['500px', '500px', 'academic', 'academic', 'accenture', 'accenture', 'adidas', 'adidas', 'adwords', 'adwords', 'airbnb', 'airbnb', 'aliexpress', 'aliexpress', 'allegro', 'allegro', 'allrecipes', 'allrecipes', 'amazon', 'amazon', 'amazon', 'amazon', 'amazon', 'amazon', 'anandtech', 'anandtech', 'android-developers', 'android-developers', 'angelfire', 'angelfire', 'apachefriends', 'apachefriends', 'api', 'api', 'apnews', 'apnews', 'armorgames', 'armorgames', 'artsandculture', 'artsandculture', 'asp', 'asp', 'attendee', 'attendee', 'audiomack', 'audiomack', 'aws', 'aws', 'bandsintown', 'bandsintown', 'banggood', 'banggood', 'bhphotovideo', 'bhphotovideo', 'binance', 'binance', 'bitcointalk', 'bitcointalk', 'bitfinex', 'bitfinex', 'bitly', 'bitly', 'blogger', 'blogger', 'blog', 'blog', 'blog', 'blog', 'bloomberg', 'bloomberg', 'blubrry', 'blubrry', 'boingboing', 'boingboing', 'brainoff', 'brainoff', 'brave', 'brave', 'brew', 'brew', 'buildyourfuture', 'buildyourfuture', 'bungie', 'bungie', 'businesswire', 'businesswire', 'bustle', 'bustle', 'calendly', 'calendly', 'chicagotribune', 'chicagotribune', 'cincinnati', 'cincinnati', 'cisco', 'cisco', 'citrix', 'citrix', 'cleveland', 'cleveland', 'clickfunnels', 'clickfunnels', 'cloudup', 'cloudup', 'cloud', 'cloud', 'cnbc', 'cnbc', 'code', 'code', 'coinmarketcap', 'coinmarketcap', 'commondreams', 'commondreams', 'complex', 'complex', 'creativecommons', 'creativecommons', 'crunchbase', 'crunchbase', 'cse', 'cse', 'cs', 'cs', 'datastudio', 'datastudio', 'deondernemer', 'deondernemer', 'developer', 'developer', 'dignitymemorial', 'dignitymemorial', 'discogs', 'discogs', 'disqus', 'disqus', 'docs', 'docs', 'docs', 'docs', 'docs', 'docs', 'dropbox', 'dropbox', 'edx', 'edx', 'eff', 'eff', 'endomondo', 'endomondo', 'eonline', 'eonline', 'esquire', 'esquire', 'example', 'example', 'familysearch', 'familysearch', 'fao', 'fao', 'flattr', 'flattr', 'flickr', 'flickr', 'flipboard', 'flipboard', 'foxnews', 'foxnews', 'freepik', 'freepik', 'freesound', 'freesound', 'fs', 'fs', 'g2a', 'g2a', 'genius', 'genius', 'getbootstrap', 'getbootstrap', 'getpostman', 'getpostman', 'gitlab', 'gitlab', 'gleam', 'gleam', 'global', 'global', 'googleads', 'googleads', 'googleprojectzero', 'googleprojectzero', 'go', 'go', 'g', 'g', 'headlines', 'headlines', 'heavy', 'heavy', 'heise', 'heise', 'hollywoodreporter', 'hollywoodreporter', 'homedepot', 'homedepot', 'hotmart', 'hotmart', 'httrack', 'httrack', 'husqvarna', 'husqvarna', 'ign', 'ign', 'iheart', 'iheart', 'ikea', 'ikea', 'imdb', 'imdb', 'imdb', 'imdb', 'independent', 'independent', 'independent', 'independent', 'informit', 'informit', 'ipoteka', 'ipoteka', 'irishtimes', 'irishtimes', 'ja', 'ja', 'knowyourmeme', 'knowyourmeme', 'kritzelpixel', 'kritzelpixel', 'later', 'later', 'lazada', 'lazada', 'lds', 'lds', 'leanpub', 'leanpub', 'lemonde', 'lemonde', 'lg', 'lg', 'linode', 'linode', 'loudwire', 'loudwire', 'lucene', 'lucene', 'marriott', 'marriott', 'marxists', 'marxists', 'mayoclinic', 'mayoclinic', 'mbsy', 'mbsy', 'mcafee', 'mcafee', 'mediafire', 'mediafire', 'meetup', 'meetup', 'minds', 'minds', 'miniclip', 'miniclip', 'mit', 'mit', 'mi', 'mi', 'mondaq', 'mondaq', 'motherboard', 'motherboard', 'moz', 'moz', 'mx', 'mx', 'mythemeshop', 'mythemeshop', 'm', 'm', 'networkadvertising', 'networkadvertising', 'news', 'news', 'news', 'news', 'nielsen', 'nielsen', 'nike', 'nike', 'notepad-plus', 'notepad-plus', 'novoresume', 'novoresume', 'npr', 'npr', 'nps', 'nps', 'nydailynews', 'nydailynews', 'ocregister', 'ocregister', 'ohsheglows', 'ohsheglows', 'ok', 'ok', 'packtpub', 'packtpub', 'panasonic', 'panasonic', 'parliament', 'parliament', 'periscope', 'periscope', 'pinterest', 'pinterest', 'player', 'player', 'plurk', 'plurk', 'pl', 'pl', 'pmindia', 'pmindia', 'pnas', 'pnas', 'poetryfoundation', 'poetryfoundation', 'pond5', 'pond5', 'prestashop', 'prestashop', 'privacy', 'privacy', 'promo', 'promo', 'pse', 'pse', 'psycnet', 'psycnet', 'radiolab', 'radiolab', 'raspberrypi', 'raspberrypi', 'razorpay', 'razorpay', 'readyforboarding', 'readyforboarding', 'redcross', 'redcross', 'researchgate', 'researchgate', 'rp-online', 'rp-online', 'rpp', 'rpp', 'samsung', 'samsung', 'scratch', 'scratch', 'scripts', 'scripts', 'sec', 'sec', 'sephora', 'sephora', 'shazam', 'shazam', 'shop', 'shop', 'site', 'site', 'skift', 'skift', 'sky', 'sky', 'smashingmagazine', 'smashingmagazine', 'smh', 'smh', 'ssl', 'ssl', 'steamcommunity', 'steamcommunity', 'stitcher', 'stitcher', 'streetvoice', 'streetvoice', 'stripe', 'stripe', 'sublimetext', 'sublimetext', 'sueddeutsche', 'sueddeutsche', 'tabelog', 'tabelog', 'taringa', 'taringa', 'techcrunch', 'techcrunch', 'technologyreview', 'technologyreview', 'technorati', 'technorati', 'tecnoblog', 'tecnoblog', 'tested', 'tested', 'themeforest', 'themeforest', 'thumbtack', 'thumbtack', 'ticketsource', 'ticketsource', 'tinypic', 'tinypic', 'treasury', 'treasury', 'tudou', 'tudou', 'tunein', 'tunein', 'tw', 'tw', 't', 't', 'ub', 'ub', 'udemy', 'udemy', 'unsplash', 'unsplash', 'uspto', 'uspto', 'vaccines', 'vaccines', 'vg', 'vg', 'vimaorthodoxias', 'vimaorthodoxias', 'vox', 'vox', 'vr', 'vr', 'vulture', 'vulture', 'wall', 'wall', 'waze', 'waze', 'welt', 'welt', 'who', 'who', 'windowsphone', 'windowsphone', 'wired', 'wired', 'wix', 'wix', 'wolframalpha', 'wolframalpha', 'youtu', 'youtu', 'yummly', 'yummly', 'zara', 'zara', 'zdf', 'zdf', 'zoho', 'zoho']\n",
      "Error: Labels contain non-numeric characters or are not convertible to integers.\n",
      "Labels after conversion: [0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9, 9, 9, 9, 9, 10, 10, 11, 11, 12, 12, 13, 13, 14, 14, 15, 15, 16, 16, 17, 17, 18, 18, 19, 19, 20, 20, 21, 21, 22, 22, 23, 23, 24, 24, 25, 25, 26, 26, 27, 27, 28, 28, 29, 29, 30, 30, 30, 30, 31, 31, 32, 32, 33, 33, 34, 34, 35, 35, 36, 36, 37, 37, 38, 38, 39, 39, 40, 40, 41, 41, 42, 42, 43, 43, 44, 44, 45, 45, 46, 46, 47, 47, 48, 48, 49, 49, 50, 50, 51, 51, 52, 52, 53, 53, 54, 54, 55, 55, 56, 56, 57, 57, 58, 58, 59, 59, 60, 60, 61, 61, 62, 62, 63, 63, 64, 64, 65, 65, 65, 65, 65, 65, 66, 66, 67, 67, 68, 68, 69, 69, 70, 70, 71, 71, 72, 72, 73, 73, 74, 74, 75, 75, 76, 76, 77, 77, 78, 78, 79, 79, 80, 80, 81, 81, 82, 82, 83, 83, 84, 84, 85, 85, 86, 86, 87, 87, 88, 88, 89, 89, 90, 90, 91, 91, 92, 92, 93, 93, 94, 94, 95, 95, 96, 96, 97, 97, 98, 98, 99, 99, 100, 100, 101, 101, 102, 102, 103, 103, 104, 104, 104, 104, 105, 105, 105, 105, 106, 106, 107, 107, 108, 108, 109, 109, 110, 110, 111, 111, 112, 112, 113, 113, 114, 114, 115, 115, 116, 116, 117, 117, 118, 118, 119, 119, 120, 120, 121, 121, 122, 122, 123, 123, 124, 124, 125, 125, 126, 126, 127, 127, 128, 128, 129, 129, 130, 130, 131, 131, 132, 132, 133, 133, 134, 134, 135, 135, 136, 136, 137, 137, 138, 138, 139, 139, 139, 139, 140, 140, 141, 141, 142, 142, 143, 143, 144, 144, 145, 145, 146, 146, 147, 147, 148, 148, 149, 149, 150, 150, 151, 151, 152, 152, 153, 153, 154, 154, 155, 155, 156, 156, 157, 157, 158, 158, 159, 159, 160, 160, 161, 161, 162, 162, 163, 163, 164, 164, 165, 165, 166, 166, 167, 167, 168, 168, 169, 169, 170, 170, 171, 171, 172, 172, 173, 173, 174, 174, 175, 175, 176, 176, 177, 177, 178, 178, 179, 179, 180, 180, 181, 181, 182, 182, 183, 183, 184, 184, 185, 185, 186, 186, 187, 187, 188, 188, 189, 189, 190, 190, 191, 191, 192, 192, 193, 193, 194, 194, 195, 195, 196, 196, 197, 197, 198, 198, 199, 199, 200, 200, 201, 201, 202, 202, 203, 203, 204, 204, 205, 205, 206, 206, 207, 207, 208, 208, 209, 209, 210, 210, 211, 211, 212, 212, 213, 213, 214, 214, 215, 215, 216, 216, 217, 217, 218, 218, 219, 219, 220, 220, 221, 221, 222, 222, 223, 223, 224, 224, 225, 225, 226, 226, 227, 227, 228, 228, 229, 229, 230, 230, 231, 231, 232, 232]\n",
      "Number of training samples: 385\n",
      "Number of validation samples: 97\n",
      "Number of test samples: 97\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "# Define the path to your dataset\n",
    "dataset_path = r'C:\\Users\\suraj\\OneDrive\\Desktop\\banao_ai\\Website Screenshots.v1-raw.yolov4pytorch\\valid'\n",
    "\n",
    "# Placeholder lists for images and labels\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "# Iterate through your dataset directory\n",
    "for filename in os.listdir(dataset_path):\n",
    "    if filename.endswith('.jpg'):\n",
    "        # Assuming image filenames contain the corresponding label (e.g., 'label_image.jpg')\n",
    "        label = filename.split('_')[0]  # Modify this based on your naming convention\n",
    "        image_path = os.path.join(dataset_path, filename)\n",
    "        \n",
    "        # Load the image using OpenCV\n",
    "        image = cv2.imread(image_path)\n",
    "        \n",
    "        # Append the image and label to the lists\n",
    "        images.append(image)\n",
    "        labels.append(label)\n",
    "\n",
    "# Verify the loaded data\n",
    "assert len(images) == len(labels), \"Mismatched number of images and labels\"\n",
    "# Assuming you have lists of images and corresponding labels\n",
    "# Replace [...] with your actual dataset loading code\n",
    "\n",
    "# Verify the loaded data\n",
    "assert len(images) == len(labels), \"Mismatched number of images and labels\"\n",
    "\n",
    "# Print the labels before conversion\n",
    "print(\"Labels before conversion:\", labels)\n",
    "\n",
    "# Convert labels to integers if they are not already\n",
    "try:\n",
    "    labels = [int(label.split('px')[0]) for label in labels]\n",
    "except ValueError:\n",
    "    print(\"Error: Labels contain non-numeric characters or are not convertible to integers.\")\n",
    "\n",
    "# Convert labels to integers if they are numeric, otherwise assign a unique integer to non-numeric labels\n",
    "label_dict = {}\n",
    "numeric_labels = []\n",
    "for label in labels:\n",
    "    try:\n",
    "        numeric_label = int(label)\n",
    "        numeric_labels.append(numeric_label)\n",
    "    except ValueError:\n",
    "        if label not in label_dict:\n",
    "            label_dict[label] = len(label_dict)\n",
    "        numeric_labels.append(label_dict[label])\n",
    "\n",
    "# Print the labels after conversion\n",
    "print(\"Labels after conversion:\", numeric_labels)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "num_classes = len(set(numeric_labels))\n",
    "labels_one_hot = to_categorical(numeric_labels, num_classes=num_classes)\n",
    "\n",
    "\n",
    "\n",
    "# # Print the labels after conversion\n",
    "# print(\"Labels after conversion:\", labels)\n",
    "\n",
    "# # Convert labels to one-hot encoding\n",
    "# num_classes = len(set(labels))\n",
    "# labels_one_hot = to_categorical(labels, num_classes=num_classes)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(images, labels_one_hot, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the sizes of each set\n",
    "print(f\"Number of training samples: {len(X_train)}\")\n",
    "print(f\"Number of validation samples: {len(X_val)}\")\n",
    "print(f\"Number of test samples: {len(y_val)}\")\n",
    "\n",
    "\n",
    "\n",
    "# Continue with the rest of your code...\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 766, 1022, 32)     896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 383, 511, 32)      0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 381, 509, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 190, 254, 64)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 3088640)           0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               395346048 \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8)                 1032      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 395366472 (1.47 GB)\n",
      "Trainable params: 395366472 (1.47 GB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Evaluation:\n",
    "\n",
    "Evaluate your trained model on the validation set to assess its generalization performance. Use metrics relevant to your task, such as accuracy, precision, recall, or others.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 33s 3s/step\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "# Assuming X_val is your validation data\n",
    "y_pred = model.predict(np.array(X_val))\n",
    "\n",
    "# Assuming y_val is one-hot encoded, convert it back to labels if needed\n",
    "y_true_labels = np.argmax(y_val, axis=1)\n",
    "\n",
    "# Compute metrics\n",
    "accuracy = accuracy_score(y_true_labels, y_true_labels)\n",
    "precision = precision_score(y_true_labels, y_true_labels, average='weighted')\n",
    "recall = recall_score(y_true_labels, y_true_labels, average='weighted')\n",
    "f1 = f1_score(y_true_labels, y_true_labels, average='weighted')\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Set Evaluation:\n",
    "\n",
    "Once satisfied with the model's performance on the validation set, evaluate it on the test set to obtain a final assessment of its generalization capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load your saved model\n",
    "your_model = load_model(r'C:\\Users\\suraj\\OneDrive\\Desktop\\banao_ai\\your_model.h5')\n",
    "\n",
    "# Enable eager execution\n",
    "your_model.compile(run_eagerly=True)\n",
    "\n",
    "class_names = ['button', 'field', 'heading', 'iframe', 'image', 'label', 'link', 'text']\n",
    "\n",
    "# Sample annotation file format: Assuming CSV format with columns: image_path, xmin, ymin, xmax, ymax, class_label\n",
    "annotation_file = r'C:\\Users\\suraj\\OneDrive\\Desktop\\banao_ai\\Website Screenshots.v1-raw.yolov4pytorch\\test\\_annotations.txt'\n",
    "\n",
    "# Read the annotation file\n",
    "annotations = []\n",
    "with open(annotation_file, 'r') as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split(',')\n",
    "        annotations.append({\n",
    "            'image_path': parts[0],\n",
    "            'xmin': int(parts[1]),\n",
    "            'ymin': int(parts[2]),\n",
    "            'xmax': int(parts[3]),\n",
    "            'ymax': int(parts[4].replace(\" \", \"\")),  # Remove spaces and convert to int\n",
    "            'class_label': int(parts[5])  # Assuming class labels are integers\n",
    "        })\n",
    "\n",
    "# Load your test data (test_image_paths)\n",
    "test_image_paths = [annotation['image_path'] for annotation in annotations]\n",
    "\n",
    "for image_path, annotation in zip(test_image_paths, annotations):\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is not None:\n",
    "        img = img / 255.0\n",
    "        img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "        predictions = your_model.predict(img)\n",
    "\n",
    "        print(f\"Image: {image_path}\")\n",
    "        print(\"Annotations:\", annotation)\n",
    "        print(\"Predictions shape:\", predictions.shape)\n",
    "\n",
    "        if len(predictions) > 0:\n",
    "            print(\"First prediction:\", predictions[0])\n",
    "            confidence_threshold = 0.5\n",
    "            filtered_predictions = [pred for pred in predictions[0] if pred[4] >= confidence_threshold]\n",
    "            print(\"Filtered Predictions shape:\", np.array(filtered_predictions).shape)\n",
    "\n",
    "        # Set a confidence threshold\n",
    "        confidence_threshold = 0.5\n",
    "\n",
    "        # Draw bounding boxes and labels on the image\n",
    "        for detection in predictions[0]:\n",
    "            confidence = detection[0]  # Assuming confidence is in the first position in the array\n",
    "            if confidence > confidence_threshold:\n",
    "                class_index = np.argmax(detection[1:])  # Assuming the class prediction is the highest probability\n",
    "                class_name = class_names[class_index]\n",
    "\n",
    "                # Get the bounding box coordinates from the annotation\n",
    "                xmin, ymin, xmax, ymax = annotation['xmin'], annotation['ymin'], annotation['xmax'], annotation['ymax']\n",
    "\n",
    "                # Draw bounding box\n",
    "                cv2.rectangle(img, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n",
    "\n",
    "                # Draw label\n",
    "                label = f'{class_name}: {confidence:.2f}'\n",
    "                cv2.putText(img, label, (xmin, ymin - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "        # Display the image\n",
    "        plt.imshow(img[0])  # Remove batch dimension before displaying\n",
    "        plt.axis('off')\n",
    "        plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
